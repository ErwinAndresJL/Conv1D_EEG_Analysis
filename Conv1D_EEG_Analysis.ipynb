{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv1D_EEG_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeKBJCf4dBMu"
      },
      "source": [
        "!pip install tensorflow-gpu==2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDkzSE4gdKus"
      },
      "source": [
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tables\n",
        "from sklearn import preprocessing\n",
        "\n",
        "TEST_SIZE = 0.1\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "def make_data(data_lists, data_type, filepath):\n",
        "  data_dict = {}\n",
        "  h5file = tables.open_file(filepath, mode=\"r+\")\n",
        "  for group_type in data_lists:\n",
        "    key_list = data_lists[group_type]\n",
        "    for i, part_id in enumerate(key_list):\n",
        "      if i == 0:\n",
        "        data = h5file.get_node('/'+part_id+'/'+data_type)[:]\n",
        "      else:\n",
        "        data = np.concatenate([data, h5file.get_node('/'+part_id+'/'+data_type)[:]])\n",
        "    data_dict[group_type] = data\n",
        "  return data_dict\n",
        "\n",
        "def load_and_prep(FILE_PATH):\n",
        "  f = h5py.File(FILE_PATH, 'r')\n",
        "  key_list = list(f.keys())\n",
        "\n",
        "  train_list, test_list = train_test_split(key_list, test_size=TEST_SIZE, \n",
        "                                          random_state=RANDOM_STATE)\n",
        "  train_list, val_list = train_test_split(train_list, test_size=TEST_SIZE, \n",
        "                                          random_state=RANDOM_STATE)\n",
        "  data_lists = {'train': train_list, \n",
        "                'test': test_list,\n",
        "                'val': val_list}\n",
        "\n",
        "  data_x_dict = make_data(data_lists, 'Data_x', FILE_PATH)\n",
        "  X_train = data_x_dict['train']\n",
        "  X_val = data_x_dict['val']\n",
        "  X_test = data_x_dict['test']\n",
        "\n",
        "  data_y_dict = make_data(data_lists, 'Data_y', FILE_PATH)\n",
        "  y_train = data_y_dict['train']\n",
        "  y_val = data_y_dict['val']\n",
        "  y_test = data_y_dict['test']\n",
        "\n",
        "  # lets change the integers so they are ordered\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  y_train = le.fit_transform(y_train)\n",
        "  y_val = le.transform(y_val)\n",
        "  y_test = le.transform(y_test)\n",
        "\n",
        "  print(X_train.shape)\n",
        "  print(X_val.shape)\n",
        "  print(X_test.shape)\n",
        "\n",
        "  print(y_train.shape)\n",
        "  print(y_val.shape)\n",
        "  print(y_test.shape)\n",
        "\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "X_train_filt, X_val_filt, X_test_filt, y_train_filt, y_val_filt, y_test_filt = load_and_prep(FILT_FILE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a1wBV_0dPDX"
      },
      "source": [
        "Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQSPf3o6dQHr"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Input\n",
        "from tensorflow.keras.backend import clear_session\n",
        "\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "# this just tells the model what input shape to expect\n",
        "model.add(Input(shape=X_train_filt.shape[1:]))\n",
        "for i in range(2):\n",
        "  model.add(Conv1D(filters=64,\n",
        "                  kernel_size=3,\n",
        "                  padding=\"same\",\n",
        "                  activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxZw7LxwdTDV"
      },
      "source": [
        "Maxpooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp4RmlCndUm2"
      },
      "source": [
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "model.add(MaxPooling1D(pool_size=3, # size of the window\n",
        "                       strides=2,   # factor to downsample\n",
        "                       padding='same'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH-vkmosdXqs"
      },
      "source": [
        "for i in range(2):\n",
        "  model.add(Conv1D(filters=128,\n",
        "                  kernel_size=3,\n",
        "                  padding=\"same\",\n",
        "                  activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kNn63vxdb2V"
      },
      "source": [
        "Reduce the output of the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as1cAZqLdexY"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "model.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0vCIy8dh5B"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.add(Dense(units=100,\n",
        "                activation='relu'))\n",
        "\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'AUC', 'Recall', 'Precision'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHwGA8wVdnk_"
      },
      "source": [
        "Add Dense Layers and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQUFyKLFdp14"
      },
      "source": [
        "# Returns a short sequential model\n",
        "def create_model(input_shape, flatten=False):\n",
        "  clear_session()\n",
        "  model = Sequential()\n",
        "\n",
        "  # this just tells the model what input shape to expect\n",
        "  model.add(Input(shape=input_shape[1:]))\n",
        "  for i in range(2):\n",
        "    model.add(Conv1D(filters=64,\n",
        "                    kernel_size=3,\n",
        "                    padding=\"same\",\n",
        "                    activation='relu'))\n",
        "    \n",
        "  model.add(MaxPooling1D(pool_size=3, # size of the window\n",
        "                       strides=2,   # factor to downsample\n",
        "                       padding='same'))\n",
        "  \n",
        "  for i in range(2):\n",
        "    model.add(Conv1D(filters=128,\n",
        "                    kernel_size=3,\n",
        "                    padding=\"same\",\n",
        "                    activation='relu'))\n",
        "  if flatten:\n",
        "    model.add(Flatten())\n",
        "  else:\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "  model.add(Dense(units=64,\n",
        "                  activation='relu'))\n",
        "\n",
        "  model.add(Dense(units=1,\n",
        "                  activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer=Adam(0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', 'AUC', 'Recall', 'Precision'])\n",
        "\n",
        "  return model\n",
        "\n",
        "clear_session()\n",
        "# Create a basic model instance\n",
        "model = create_model(X_train_filt.shape)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiDyVSqAdtvx"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "def create_callbacks(best_model_filepath, tensorboard_logs_filepath):\n",
        "\n",
        "  callback_checkpoint = ModelCheckpoint(filepath=best_model_filepath,\n",
        "                                        monitor='val_loss',\n",
        "                                        verbose=0,\n",
        "                                        save_weights_only=True,\n",
        "                                        save_best_only=True)\n",
        "  \n",
        "  callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                          patience=10, \n",
        "                                          verbose=1)\n",
        "  \n",
        "  callback_tensorboard = TensorBoard(log_dir=tensorboard_logs_filepath,\n",
        "                                     histogram_freq=0,\n",
        "                                     write_graph=False)\n",
        "  \n",
        "  callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                         factor=0.1,\n",
        "                                         min_lr=1e-4,\n",
        "                                         patience=0,\n",
        "                                         verbose=1)\n",
        "  \n",
        "  return [callback_checkpoint, callback_early_stopping,\n",
        "          callback_tensorboard, callback_reduce_lr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9gsl8AndwSx"
      },
      "source": [
        "train the model .."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h0rjdLCdxmS"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "best_model_filepath = \"CNN1D_Model.ckpt\"\n",
        "tensorboard_logs_filepath = \"./CNN1D_logs/\"\n",
        "\n",
        "# calculate the class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                  np.unique(y_train_filt),\n",
        "                                                  y_train_filt)\n",
        "\n",
        "history_1D = model.fit(X_train_filt, \n",
        "                       y_train_filt,\n",
        "                       batch_size=BATCH_SIZE, \n",
        "                       epochs=EPOCHS,\n",
        "                       validation_data = (X_val_filt, y_val_filt),\n",
        "                       callbacks= create_callbacks(best_model_filepath, \n",
        "                                                   tensorboard_logs_filepath),\n",
        "                       class_weight = class_weights,\n",
        "                       verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}